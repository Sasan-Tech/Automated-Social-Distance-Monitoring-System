{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six.moves.urllib as urllib\n",
    "import glob\n",
    "import itertools\n",
    "import zipfile\n",
    "import matplotlib\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from numba import jit, cuda\n",
    "\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Path & Other Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_OUTPUT_SIZE = (12, 8)\n",
    "SAFE_DISTANCE = 100 #in pixels\n",
    "\n",
    "#Frozen Graph of trained models (rfcn_resnet101_coco_2018_01_28, ssd_enceptionv2, ssd_mobilenet_v1_coco_2018_01_28)\n",
    "FROZEN_RFCN_GRAPH = \"frozen_inference_graph.pb\" #define the model path here\n",
    "#FROZEN_SSD_INCEPTION_GRAPH = \"ssd_inception_exported/frozen_inference_graph.pb\"\n",
    "#FROZEN_SSD_MOBILENET = \"ssd_mobilenet_v1_coco_exported/frozen_inference_graph.pb\"\n",
    "\n",
    "frozen_graph = FROZEN_RFCN_GRAPH  #define the chosen model\n",
    "label_map = label_map_util.load_labelmap(\"label_map.pbtxt\")  #define the label_map.pbtxt path here\n",
    "\n",
    "cap = cv2.VideoCapture('TownCentreXVID.mp4')  #define the video path here\n",
    "\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "dim = (width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the frozen graph and label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "  graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(frozen_graph, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=1, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of Calculating distance between centroids of each predicted boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to numpy array\n",
    "def image_to_numpy_array(image):\n",
    "  (real_width, real_height) = image.size\n",
    "  return np.array(image.getdata()).reshape((real_height, real_width, 3)).astype(np.uint8)\n",
    "\n",
    "# Get the coordinates for each object\n",
    "def calculate_coordinates(box, width, height):\n",
    "  xmin = box[1] * width\n",
    "  ymin = box[0] * height\n",
    "  xmax = box[3] * width\n",
    "  ymax = box[2] * height\n",
    "  return [xmin, ymin, xmax - xmin, ymax - ymin]\n",
    "\n",
    "# Calculate all possible permutations between centroids\n",
    "def calculate_permutation(centroids):\n",
    "  permutations = []\n",
    "  for permutation in itertools.permutations(centroids, 2):\n",
    "    if permutation[::-1] not in permutations:\n",
    "      permutations.append(permutation)\n",
    "  return permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting person objects in each frame and displaying it as a real time video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "  with detection_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Definite input and output Tensors for detection_graph\n",
    "      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        \n",
    "      # Each box represents a part of the image where a particular object was detected.\n",
    "      detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        \n",
    "      # Each score represent how level of confidence for each of the objects.\n",
    "      # Score is shown on the result image, together with the class label.\n",
    "      detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "      detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "      num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "  \n",
    "      tope = 10;\n",
    "      limit = 0;\n",
    "      new = True;\n",
    "      while(cap.isOpened()):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read();\n",
    "        \n",
    "        if ret == True:\n",
    "            # Correct color\n",
    "            frame = gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            operations = tf.get_default_graph().get_operations()\n",
    "            tensor_names = {output.name for operation in operations for output in operation.outputs}\n",
    "            tensor_dict = {}\n",
    "            \n",
    "            for key in ['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes', 'detection_masks']:\n",
    "                tensor = key + \":0\"\n",
    "                if tensor in tensor_names:\n",
    "                  tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor)\n",
    "            \n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['num_detections'], [0])\n",
    "                num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0,0], [num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [num_detection, -1, -1])\n",
    "                detection_masks = util_ops.reframe_box_masks_to_image_masks(detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks = tf.cast(tf.greater(detection_masks, 0.5), tf.uint8)\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(detection_masks, 0)\n",
    "\n",
    "            # Actual detection.\n",
    "            output_dict = sess.run(tensor_dict, feed_dict={image_tensor: np.expand_dims(frame, 0)}) #The image is from parameter\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "        \n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "            \n",
    "            # Calculate normalized coordinates for boxes\n",
    "            centroids = []\n",
    "            coordinates = []\n",
    "            for box in output_dict['detection_boxes']:\n",
    "                coordinate = calculate_coordinates(box, width, height)\n",
    "\n",
    "                #[xmin, ymin, xmax - xmin, ymax - ymin]\n",
    "                centroid = (coordinate[0] + (coordinate[2]/2), coordinate[1] + (coordinate[3]/2))\n",
    "                centroids.append(centroid)\n",
    "                coordinates.append(coordinate)\n",
    "          \n",
    "            permutations = calculate_permutation(centroids)\n",
    "            \n",
    "            # Display boxes and centroids\n",
    "            fig, axis = plt.subplots(figsize = (10, 6), dpi=90, frameon=False)\n",
    "            for coordinate, centroid in zip(coordinates, centroids):\n",
    "                plt.axis('off')\n",
    "                axis.add_patch(matplotlib.patches.Rectangle((coordinate[0], coordinate[1]), coordinate[2], coordinate[3], linewidth=1, edgecolor='yellow', facecolor='none', zorder=10))\n",
    "                axis.add_patch(matplotlib.patches.Circle((centroid[0], centroid[1]), 3, color='yellow', zorder=20))\n",
    "\n",
    "            # Display lines between centroids\n",
    "            for permutation in permutations:\n",
    "                x1 = permutation[0][0]\n",
    "                x2 = permutation[1][0]\n",
    "                y1 = permutation[0][1]\n",
    "                y2 = permutation[1][1]\n",
    "                \n",
    "                # Calculate the distance between 2 centroids\n",
    "                distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "                average_distance = distance/SAFE_DISTANCE\n",
    "                \n",
    "                # Calculate middle point\n",
    "                middle = ((x1 + x2)/2, (y1 + y2)/2)\n",
    "                \n",
    "                # Calculate slope\n",
    "                if(x2-x1 > 0):\n",
    "                    slope = (y2-y1)/(x2-x1)\n",
    "                else:\n",
    "                    slope = (y2-y1)\n",
    "                    \n",
    "                dy = math.sqrt(3**2 / (slope**2 + 1))\n",
    "                dx = -slope * dy\n",
    "                \n",
    "                # Set random location\n",
    "                if random.randint(1, 10) % 2 == 0:\n",
    "                  dx = middle[0] - dx*10\n",
    "                  dy = middle[1] - dy*10\n",
    "                else:\n",
    "                  dx = middle[0] + dx*10\n",
    "                  dy = middle[1] + dy*10\n",
    "                    \n",
    "                if average_distance < 2:\n",
    "                  axis.annotate((round(average_distance, 2)), xy=middle, color='white', xytext=(dx, dy), fontsize=10, bbox=dict(facecolor='red', edgecolor='white', boxstyle='round', pad=0.2), zorder=30)\n",
    "                  axis.plot((x1, x2), (y1, y2), linewidth=2, color='red', zorder=15)\n",
    "                else:\n",
    "                  pass\n",
    "            \n",
    "            axis.imshow(frame, interpolation='nearest')\n",
    "            \n",
    "            fig.canvas.draw() # Convert figure to numpy\n",
    "            \n",
    "            img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "            img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            \n",
    "            img = np.array(fig.canvas.get_renderer()._renderer)\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Display frames continuosly until 'esc' button has been pressed\n",
    "            cv2.imshow('LIVE', img)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            #Press esc key to stop the real time video\n",
    "            if(key == 27):\n",
    "                break\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the program. Press 'esc' key on your keyboard to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ff364026530b>:69: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axis = plt.subplots(figsize = (10, 6), dpi=90, frameon=False)\n",
      "<ipython-input-14-ff364026530b>:116: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n"
     ]
    }
   ],
   "source": [
    "#run the program\n",
    "run_inference()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
